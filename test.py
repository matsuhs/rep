# -*- coding: utf-8 -*-
"""2_LogisticRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GOG7YOBtNLtEdD7UiX3XPnBt7jSqHL9j
"""

!pip install --upgrade pip
!pip install mecab-python3
!pip install unidic-lite
!pip install pymysql
!pip install scipy
!pip install numpy
!pip install -U imbalanced-learn

import urllib.request
import os
import gzip
import shutil
import boto3
import pandas as pd
import matplotlib.pyplot as plt
import MeCab
import numpy as np

s3_bucket="matsuhs-dmix"
s3_object="review_label/review_data.csv"
file_name = "amazon_review.csv"
dir_name="tmp"
file_path = os.path.join(dir_name,file_name)
data_prefix="input_data"


my_random_state=9876


# os.makedirs(dir_name, exist_ok=True)

# if os.path.exists(file_path):
#     print("File {} already exists. Skipped download.".format(file_name))
# else:
#     # urllib.request.urlretrieve(download_url, file_path)
#     s3 = boto3.resource('s3')
#     s3.Object(s3_bucket, s3_object).download_file(file_path)
#     print("File downloaded: {}".format(file_path))
    
# if os.path.exists(tsv_file_path):
#     print("File {} already exists. Skipped unzip.".format(tsv_file_name))
# else:
#     with gzip.open(file_path, mode='rb') as fin:
#         with open(tsv_file_path, 'wb') as fout:
#             shutil.copyfileobj(fin, fout)
#             print("File uznipped: {}".format(tsv_file_path))

import pymysql

# sql = """
# SELECT
#  r.marketplace,
#  r.customer_id,
#  r.review_id,
#  r.product_id,
#  r.product_parent,
#  r.product_title,
#  r.product_category,
#  r.star_rating,
#  r.helpful_votes,
#  r.total_votes,
#  r.vine,
#  r.verified_purchase,
#  r.review_headline,
#  r.review_body,
#  r.review_date,
#  p.label
# FROM review AS r
# INNER JOIN product AS p
# ON r.product_id = p.product_id 
# ;
# """


sql = """
SELECT
 r.product_id,
 r.product_parent,
 r.customer_id,
 r.num_review, 
 r.star5_ratio,
 r.star1_ratio,
 r.sum_helpful,
 r.sum_negative,
 r.num_vine,
 r.purchase_ratio,
 r.std_date,
 s.label
FROM review2 AS r
INNER JOIN score AS s
ON r.product_id = s.product_id 
"""

connection = pymysql.connect(
    host='scraping.cluster-cxopxgqs0cbo.ap-northeast-1.rds.amazonaws.com',
    user='admin',
    passwd='Tra1nsp0tt1ng',
    db='mydb',
    charset='utf8')

try:
    with connection.cursor() as cursor:
        cursor.execute(sql)
        rows = cursor.fetchall()

        new_result = [one for one in rows]
        df = pd.DataFrame(new_result)
#         df.columns = [
#             "marketplace","customer_id","review_id","product_id",
#             "product_parent","product_title","product_category",
#             "star_rating","helpful_votes","total_votes","vine",
#             "verified_purchase","review_headline","review_body",
#             "review_date","label"
#         ] 
        df.columns = [
            "product_id","product_parent","customer_id","num_review",
            "star5_ratio","star1_ratio","sum_helpful","sum_negative",
            "num_vine","purchase_ratio","std_date","label"
        ]
    
        
finally:
    connection.close()
    
    
df.head(2)

df.shape

x = df['label']
plt.hist(x,bins=10)

df['label'].value_counts()

df2 = df

"""## SPAMレビューの閾値を設定（定義）する"""

def deceptive_flag (label_score):
    flag = 0 # Truthefull
    if label_score >= 40:
        flag = 1 # Deceptive
    return flag

df2['deceptive'] = df2['label'].apply(deceptive_flag)
df2['deceptive'].value_counts()

#不均衡データ : 0.48%  = 0.004808206004915055
a=df2['deceptive'].value_counts()
print(a[1]/a[0] * 100, "%")

"""# モデリング"""

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

target_col = 'deceptive'
# exclude_cols = ['deceptive','product_id', 'product_parent', 'customer_id','label']
exclude_cols = ['product_id', 'label', 'deceptive']
feature_cols = []
for col in df2.columns:
    if col not in exclude_cols:
        feature_cols.append(col)
        
feature_cols

y = df2[target_col]
X = df2[feature_cols]

X.head(2)

y[y == 1].value_counts()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=my_random_state)

len(X_train), len(X_test), len(y_train), len(y_test)

"""## Decision Tree"""

from sklearn.tree import DecisionTreeClassifier 
from sklearn.model_selection import train_test_split

dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)

y_pred[:10]

"""## 性能指標 """

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc

# confusion_matrix関数を利用し混同行列を出力
labels = [0, 1]
confusion_m = confusion_matrix(y_test, y_pred, labels=labels)
pd.DataFrame(confusion_m, columns=labels, index=labels)

# confusion_matrix関数を利用し、 true_positive, false_negative, false_positive, true_negativeを出力
true_negative, false_positive, false_negative, true_positive = confusion_matrix(y_test, y_pred).ravel()
print("true_negative： ", true_negative)
print("false_negative： ", false_negative)
print("true_positive： ", true_positive)
print("false_positive： ", false_positive)

print('Accuracy :{0:0.5f}'.format(accuracy_score(y_pred , y_test))) 
print('Precision : {0:0.5f}'.format(precision_score(y_test , y_pred)))
print('Recall : {0:0.5f}'.format(recall_score(y_test , y_pred)))
print('F1 : {0:0.5f}'.format(f1_score(y_test , y_pred)))

# ROC曲線・AUCの出力のために、.predict_proba()を使いX_testの各乗客の生存確率を予測
y_proda = dt.predict_proba(X_test)

y_proda[:10]

# roc_curve関数を利用し、ROC曲線をプロット
fpr, tpr, thresholds = roc_curve(y_test, y_proda[:,1])
plt.figure()
plt.plot(fpr, tpr, color='red', lw=2)
plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.show()

# auc関数を利用し、AUCを出力
auc(fpr, tpr)

"""# 不均衡データの扱い"""

# https://github.com/scikit-learn-contrib/imbalanced-learn
# Under-sampling
# Random majority under-sampling with replacement
# Extraction of majority-minority Tomek links [1]
# Under-sampling with Cluster Centroids
# NearMiss-(1 & 2 & 3) [2]

"""# アンダーサンプリング"""

from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler

from collections import Counter # counter takes values returns value_counts dictionary
from sklearn.datasets import make_classification

print('Original dataset shape %s' % Counter(y))

rus = RandomUnderSampler(random_state=my_random_state)
X_res, y_res = rus.fit_resample(X, y)

print('Resampled dataset shape %s' % Counter(y_res))

from sklearn.linear_model import LogisticRegression # Importing Classifier Step

X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, shuffle=True, random_state=my_random_state)

# Undersampling with Logistic Regression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)

print('Accuracy :{0:0.5f}'.format(accuracy_score(y_pred , y_test))) 
print('AUC : {0:0.5f}'.format(roc_auc_score(y_test , y_pred)))
print('Precision : {0:0.5f}'.format(precision_score(y_test , y_pred)))
print('Recall : {0:0.5f}'.format(recall_score(y_test , y_pred)))
print('F1 : {0:0.5f}'.format(f1_score(y_test , y_pred)))

plt.figure(figsize=(8,6))

fpr, tpr, thresholds = roc_curve(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)
print("AUC : ",auc,"\n")

plt.plot(fpr,tpr,linewidth=2, label="data 1, auc="+str(auc))
plt.legend(loc=4)

plt.plot([0,1], [0,1], 'k--' )

plt.rcParams['font.size'] = 12
plt.title('ROC curve for Predicting a credit card fraud detection')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')

plt.show()

from collections import Counter
from sklearn.datasets import make_classification
from imblearn.under_sampling import ClusterCentroids

print('Original dataset shape %s' % Counter(y))

cc = ClusterCentroids(random_state=my_random_state)
X_res_cc, y_res_cc = cc.fit_sample(X, y)

print('Resampled dataset shape %s' % Counter(y_res_cc))

from sklearn.linear_model import LogisticRegression # Importing Classifier Step

X_train, X_test, y_train, y_test = train_test_split(X_res_cc, y_res, test_size=0.3, shuffle=True, random_state=my_random_state)

# Undersampling with Logistic Regression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)

print('Accuracy :{0:0.5f}'.format(accuracy_score(y_pred , y_test))) 
print('AUC : {0:0.5f}'.format(roc_auc_score(y_test , y_pred)))
print('Precision : {0:0.5f}'.format(precision_score(y_test , y_pred)))
print('Recall : {0:0.5f}'.format(recall_score(y_test , y_pred)))
print('F1 : {0:0.5f}'.format(f1_score(y_test , y_pred)))

"""# オーバーサンプリング"""

from imblearn.over_sampling import RandomOverSampler

print('Original dataset shape %s' % Counter(y))

ros = RandomOverSampler(random_state=my_random_state)
X_res2, y_res2 = ros.fit_resample(X, y)

print('Resampled dataset shape %s' % Counter(y_res2))

X_train, X_test, y_train, y_test = train_test_split(X_res2, y_res2, test_size=0.3, shuffle=True, random_state=my_random_state)

# Oversampling with Logistic Regression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)

print('Accuracy :{0:0.5f}'.format(accuracy_score(y_test , y_pred))) 
print('AUC : {0:0.5f}'.format(roc_auc_score(y_test , y_pred)))
print('Precision : {0:0.5f}'.format(precision_score(y_test , y_pred)))
print('Recall : {0:0.5f}'.format(recall_score(y_test , y_pred)))
print('F1 : {0:0.5f}'.format(f1_score(y_test , y_pred)))

plt.figure(figsize=(8,6))

fpr, tpr, thresholds = roc_curve(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)
print("AUC : ",auc,"\n")

plt.plot(fpr,tpr,linewidth=2, label="data 1, auc="+str(auc))
plt.legend(loc=4)

plt.plot([0,1], [0,1], 'k--' )

plt.rcParams['font.size'] = 12
plt.title('ROC curve for Predicting a credit card fraud detection')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')

plt.show()

from imblearn.over_sampling import SMOTE
# http://glemaitre.github.io/imbalanced-learn/generated/imblearn.over_sampling.SMOTE.html#imblearn.over_sampling.SMOTE

print('Original dataset shape %s' % Counter(y))

sm = SMOTE(random_state=my_random_state)
X_ros_smote, y_ros_smote = sm.fit_sample(X, y)

print('Resampled dataset shape %s' % Counter(y_res2))

